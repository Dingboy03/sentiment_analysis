# üéØ Syst√®me d'Analyse de Sentiment avec MCP (Model Context Protocol)

## üìã Table des mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Architecture du syst√®me](#architecture-du-syst√®me)
- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [API Reference](#api-reference)
- [Exemples d'utilisation](#exemples-dutilisation)
- [D√©pannage](#d√©pannage)
- [Contribution](#contribution)

---

## üåü Vue d'ensemble

Ce projet impl√©mente un **syst√®me complet d'analyse de sentiment** pour textes en fran√ßais, bas√© sur le mod√®le **XLM-RoBERTa** fine-tun√© sur Twitter. Le syst√®me propose trois interfaces d'utilisation :

1. **Serveur MCP** : Int√©gration directe avec Claude Desktop via le Model Context Protocol
2. **API REST FastAPI** : Service HTTP pour applications web/mobiles
3. **Notebooks Jupyter** : Analyse batch et exp√©rimentation

### üéØ Cas d'usage principaux

- Analyse de sentiment de commentaires sur r√©seaux sociaux
- √âvaluation d'articles et leurs commentaires associ√©s
- D√©tection d'opinions (positive, neutre, n√©gative) avec score de confiance
- Int√©gration IA conversationnelle (Claude Desktop)

### üîë Caract√©ristiques cl√©s

‚úÖ Support de textes longs (d√©coupage automatique en chunks)  
‚úÖ Nettoyage et normalisation avanc√©s du texte  
‚úÖ Logging d√©taill√© pour debugging  
‚úÖ API REST avec documentation Swagger automatique  
‚úÖ Int√©gration MCP pour Claude Desktop  
‚úÖ Analyse batch via notebooks Jupyter  

---

## üèóÔ∏è Architecture du syst√®me

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INTERFACES UTILISATEUR                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Claude Desktop  ‚îÇ   API FastAPI    ‚îÇ  Jupyter Notebooks   ‚îÇ
‚îÇ   (MCP Client)   ‚îÇ  (HTTP REST)     ‚îÇ   (Batch Analysis)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                  ‚îÇ                    ‚îÇ
         ‚îÇ                  ‚îÇ                    ‚îÇ
         ‚ñº                  ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              COUCHE TRAITEMENT (Core Engine)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ         mcp_sentiment_server.py                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ clean_text() : Nettoyage & normalisation          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ analyze_sentiment() : Analyse NLP                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ handle_mcp_request() : Routage MCP                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  MOD√àLE NLP (Deep Learning)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  twitter-xlm-roberta-base (XLM-RoBERTa)             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Tokenizer : AutoTokenizer                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Model : AutoModelForSequenceClassification        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Classes : [negative, neutral, positive]           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîÑ Flux de donn√©es

```
Texte brut ‚Üí Nettoyage ‚Üí Tokenisation ‚Üí D√©coupage (si >512 tokens)
  ‚Üì
Analyse par chunks ‚Üí Agr√©gation ‚Üí Softmax ‚Üí Pr√©diction finale
  ‚Üì
{sentiment: "positive", confidence: 0.92}
```

---

## üì¶ Pr√©requis

### Syst√®me d'exploitation
- **Windows** 10/11 (adapt√©, mais portable sur Linux/macOS)

### Logiciels
- **Python** 3.9.25 (recommand√©, test√© avec cette version)
- **Conda** ou **Miniconda** (gestion d'environnement)
- **Claude Desktop** (optionnel, pour l'interface MCP)

### Hardware recommand√©
- **RAM** : 8 GB minimum (16 GB recommand√©)
- **Espace disque** : 2 GB pour le mod√®le + d√©pendances
- **CPU** : Processeur multi-c≈ìurs (le mod√®le tourne en CPU par d√©faut)

---

## üöÄ Installation

### 1Ô∏è‚É£ Cloner le projet

```bash
git clone <votre-repo>
cd NLP/MCP
```

### 2Ô∏è‚É£ Cr√©er l'environnement Conda

#### Option A : Depuis le fichier `environment.yml`

```bash
conda env create -f environment.yml
conda activate nlp
```

#### Option B : Installation manuelle

```bash
# Cr√©er l'environnement
conda create -n nlp python=3.9.25

# Activer l'environnement
conda activate nlp

# Installer les d√©pendances principales
pip install torch==2.8.0 transformers==4.57.3 numpy==2.0.2

# Installer les d√©pendances API
pip install fastapi==0.128.0 uvicorn==0.39.0 pydantic==2.12.5

# Installer les d√©pendances notebooks
pip install jupyter ipykernel pandas matplotlib seaborn

# Autres d√©pendances utiles
pip install requests tqdm nltk
```

### 3Ô∏è‚É£ T√©l√©charger le mod√®le

Le mod√®le doit √™tre plac√© dans :
```
C:\Users\HP ZBOOK\Desktop\ETUDES\2024-2025\NLP\models\twitter-xlm-roberta
```

**T√©l√©chargement depuis Hugging Face** :

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

MODEL_NAME = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
SAVE_PATH = r"C:\Users\HP ZBOOK\Desktop\ETUDES\2024-2025\NLP\models\twitter-xlm-roberta"

# T√©l√©charger et sauvegarder
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

tokenizer.save_pretrained(SAVE_PATH)
model.save_pretrained(SAVE_PATH)
```

### 4Ô∏è‚É£ V√©rifier l'installation

Ex√©cutez le script de test :

```bash
python test_server.py
```

V√©rifiez le fichier `mcp_server.log` - vous devriez voir :
```
=== SUCC√àS ===
```

---

## ‚öôÔ∏è Configuration

### Configuration MCP pour Claude Desktop

**Fichier** : `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "sentiment-analyzer": {
      "command": "C:\\Users\\HP ZBOOK\\anaconda3\\envs\\nlp\\python.exe",
      "args": [
        "C:\\Users\\HP ZBOOK\\Desktop\\ETUDES\\2024-2025\\NLP\\MCP\\mcp_sentiment_server.py"
      ]
    }
  }
}
```

**‚ö†Ô∏è Important** :
- Utilisez des doubles backslashes `\\` dans les chemins Windows
- V√©rifiez que le chemin Python pointe vers l'environnement `nlp`
- Red√©marrez Claude Desktop apr√®s modification

### Configuration API FastAPI

L'API ne n√©cessite pas de configuration sp√©ciale, mais vous pouvez modifier :

**Port** (dans `mcp_api_server.py`) :
```python
# Par d√©faut : 8000
# Pour changer : uvicorn mcp_api_server:app --port 8080
```

**CORS** (si n√©cessaire) :
```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### Logging

Les logs sont √©crits dans :
```
C:\Users\HP ZBOOK\Desktop\ETUDES\2024-2025\NLP\MCP\mcp_server.log
```

Pour changer l'emplacement, modifiez `LOG_FILE` dans `mcp_sentiment_server.py`.

---

## üíª Utilisation

### 1Ô∏è‚É£ Serveur MCP (Claude Desktop)

#### D√©marrage

Le serveur MCP d√©marre automatiquement lors du lancement de Claude Desktop.

#### V√©rification de la connexion

1. Ouvrez Claude Desktop
2. Cliquez sur l'ic√¥ne üî® (outils) en bas √† gauche
3. V√©rifiez que `sentiment-analyzer` appara√Æt avec une pastille verte

#### Exemples de requ√™tes

```
Utilisateur : Analyse le sentiment de "Je suis super content !"

Claude : [Utilise analyze_sentiment]
R√©sultat : {
  "sentiment": "positive",
  "confidence": 0.94
}
```

```
Utilisateur : Quel est le sentiment de cet avis : "Le service est lent et le produit est d√©fectueux"

Claude : [Utilise analyze_sentiment]
R√©sultat : {
  "sentiment": "negative",
  "confidence": 0.88
}
```

### 2Ô∏è‚É£ API FastAPI

#### D√©marrage du serveur

```bash
# Activer l'environnement
conda activate nlp

# Lancer le serveur
uvicorn mcp_api_server:app --reload --host 0.0.0.0 --port 8000
```

#### Acc√©der √† la documentation

- **Swagger UI** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc

#### Tester l'API

**Avec curl** :
```bash
curl -X POST "http://localhost:8000/analyze" \
  -H "Content-Type: application/json" \
  -d '{"text": "Ce produit est excellent !"}'
```

**Avec Python** :
```python
import requests

response = requests.post(
    "http://localhost:8000/analyze",
    json={"text": "Ce produit est excellent !"}
)

print(response.json())
# {"sentiment": "positive", "confidence": 0.92}
```

### 3Ô∏è‚É£ Notebooks Jupyter

#### Lancer Jupyter

```bash
conda activate nlp
jupyter notebook
```

#### Notebooks disponibles

1. **run_analysis_with_mcp.ipynb** : Analyse batch d'articles + commentaires
2. **test_mcp_api.ipynb** : Tests de l'API REST

---

## üìÅ Structure du projet

```
NLP/MCP/
‚îÇ
‚îú‚îÄ‚îÄ mcp_sentiment_server.py      # Serveur MCP principal
‚îú‚îÄ‚îÄ mcp_api_server.py             # API REST FastAPI
‚îú‚îÄ‚îÄ test_server.py                # Script de test/diagnostic
‚îÇ
‚îú‚îÄ‚îÄ run_analysis_with_mcp.ipynb  # Notebook analyse batch
‚îú‚îÄ‚îÄ test_mcp_api.ipynb           # Notebook tests API
‚îÇ
‚îú‚îÄ‚îÄ environment.yml              # Configuration Conda
‚îú‚îÄ‚îÄ mcp_server.log               # Fichier de logs
‚îÇ
‚îú‚îÄ‚îÄ articles_commentaires_final.json        # Donn√©es d'entr√©e
‚îî‚îÄ‚îÄ analyse_sentiments_result.json          # R√©sultats d'analyse
```

### Description des fichiers

| Fichier | Description |
|---------|-------------|
| `mcp_sentiment_server.py` | C≈ìur du syst√®me : analyse NLP + serveur MCP |
| `mcp_api_server.py` | API REST avec endpoints `/analyze` et `/analyze_article` |
| `test_server.py` | Script de diagnostic pour v√©rifier l'installation |
| `run_analysis_with_mcp.ipynb` | Analyse batch de fichiers JSON |
| `test_mcp_api.ipynb` | Tests unitaires de l'API |
| `environment.yml` | D√©finition de l'environnement Conda |
| `mcp_server.log` | Logs d'ex√©cution et debugging |

---

## üìö API Reference

### Serveur MCP

#### M√©thode : `analyze_sentiment`

**Description** : Analyse le sentiment d'un texte en fran√ßais.

**Input Schema** :
```json
{
  "text": "string (required)"
}
```

**Output** :
```json
{
  "sentiment": "positive" | "neutral" | "negative",
  "confidence": 0.0-1.0
}
```

**Exemple** :
```python
# Depuis Claude Desktop
"Analyse ce texte : 'Le film √©tait fantastique !'"

# R√©ponse
{
  "sentiment": "positive",
  "confidence": 0.95
}
```

---

### API REST FastAPI

#### `POST /analyze`

**Description** : Analyse le sentiment d'un texte unique.

**Request Body** :
```json
{
  "text": "string"
}
```

**Response** :
```json
{
  "sentiment": "positive" | "neutral" | "negative",
  "confidence": 0.92
}
```

**Exemple curl** :
```bash
curl -X POST "http://localhost:8000/analyze" \
  -H "Content-Type: application/json" \
  -d '{"text": "Je d√©teste ce produit"}'
```

**R√©ponse** :
```json
{
  "sentiment": "negative",
  "confidence": 0.87
}
```

---

#### `POST /analyze_article`

**Description** : Analyse un article et ses commentaires, avec distribution des sentiments.

**Parameters** :
- `article_text` (string, required) : Texte de l'article
- `article_author` (string, optional) : Auteur de l'article (d√©faut: "Inconnu")
- `commentaires` (array, optional) : Liste des commentaires

**Commentaire Schema** :
```json
{
  "auteur": "string (optional, d√©faut: Anonyme)",
  "content": "string (required)"
}
```

**Response** :
```json
{
  "post": {
    "type": "article",
    "author": "John Doe",
    "content": "Article text...",
    "sentiment": "positive",
    "confidence": 0.85
  },
  "commentaires": [
    {
      "type": "commentaire",
      "author": "Alice",
      "content": "Super article !",
      "sentiment": "positive",
      "confidence": 0.92
    }
  ],
  "distribution": {
    "positive": 0.6,
    "neutral": 0.3,
    "negative": 0.1
  }
}
```

**Exemple Python** :
```python
import requests

payload = {
    "article_text": "Nouvel iPhone sortie aujourd'hui",
    "article_author": "Tech News",
    "commentaires": [
        {"auteur": "Alice", "content": "Trop cher !"},
        {"auteur": "Bob", "content": "J'adore le design"},
        {"content": "Bof, rien de nouveau"}
    ]
}

response = requests.post("http://localhost:8000/analyze_article", json=payload)
print(response.json())
```

---

#### `GET /`

**Description** : Message de bienvenue et informations de l'API.

**Response** :
```json
{
  "message": "Bienvenue sur l'API MCP Sentiment. POST /analyze ou /analyze_article"
}
```

---

## üß™ Exemples d'utilisation

### Exemple 1 : Analyse simple (API)

```python
import requests

url = "http://localhost:8000/analyze"

# Texte positif
response = requests.post(url, json={"text": "J'adore ce restaurant !"})
print(response.json())
# {"sentiment": "positive", "confidence": 0.94}

# Texte n√©gatif
response = requests.post(url, json={"text": "Service horrible"})
print(response.json())
# {"sentiment": "negative", "confidence": 0.89}
```

### Exemple 2 : Analyse d'article avec commentaires

```python
import requests

url = "http://localhost:8000/analyze_article"

payload = {
    "article_text": "Le nouveau smartphone est sorti avec de nouvelles fonctionnalit√©s",
    "article_author": "TechBlog",
    "commentaires": [
        {"auteur": "User1", "content": "Super, j'ai h√¢te de l'acheter !"},
        {"auteur": "User2", "content": "Trop cher pour ce qu'il propose"},
        {"content": "Int√©ressant mais je vais attendre les avis"}
    ]
}

response = requests.post(url, json=payload)
result = response.json()

print(f"Article : {result['post']['sentiment']} ({result['post']['confidence']})")
print(f"\nDistribution des commentaires :")
for sentiment, pct in result['distribution'].items():
    print(f"  {sentiment}: {pct*100:.1f}%")
```

### Exemple 3 : Analyse batch (Notebook)

```python
import json
from mcp_sentiment_server import analyze_sentiment

# Charger donn√©es
with open("articles_commentaires_final.json", "r", encoding="utf-8") as f:
    data = json.load(f)

results = []

for item in data:
    article = item["article"]
    
    # Analyser l'article
    article_result = analyze_sentiment({"text": article["contenu"]})
    
    # Analyser chaque commentaire
    for comment in item["commentaires"]:
        comment_result = analyze_sentiment({"text": comment["content"]})
        results.append({
            "article_id": article.get("id"),
            "comment_author": comment.get("auteur"),
            "sentiment": comment_result["sentiment"],
            "confidence": comment_result["confidence"]
        })

# Sauvegarder
with open("batch_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)
```

### Exemple 4 : Utilisation avec Claude Desktop

```
Vous : Analyse le sentiment de ces avis clients et dis-moi le pourcentage de chaque cat√©gorie :
1. "Produit parfait, livraison rapide"
2. "D√©√ßu de la qualit√©"
3. "Correct pour le prix"
4. "Je recommande vivement !"
5. "Service client inexistant"

Claude : [Utilise analyze_sentiment 5 fois]

Voici l'analyse des sentiments :

‚úÖ Positifs (40%) :
- Avis 1 : positive (0.95)
- Avis 4 : positive (0.92)

‚ö†Ô∏è Neutres (20%) :
- Avis 3 : neutral (0.78)

‚ùå N√©gatifs (40%) :
- Avis 2 : negative (0.85)
- Avis 5 : negative (0.91)
```

---

## üîß D√©pannage

### Probl√®me : Serveur MCP "failed" dans Claude Desktop

**Solution** :

1. V√©rifiez le fichier de log :
   ```
   C:\Users\HP ZBOOK\Desktop\ETUDES\2024-2025\NLP\MCP\mcp_server.log
   ```

2. V√©rifiez que le chemin Python est correct :
   ```bash
   where python
   # Utilisez ce chemin dans claude_desktop_config.json
   ```

3. Testez manuellement :
   ```bash
   conda activate nlp
   python test_server.py
   ```

4. V√©rifiez les permissions :
   - Le dossier doit √™tre accessible en lecture/√©criture
   - D√©sactivez temporairement l'antivirus si n√©cessaire

### Probl√®me : API FastAPI ne d√©marre pas

**Erreur** : `ModuleNotFoundError: No module named 'fastapi'`

**Solution** :
```bash
conda activate nlp
pip install fastapi uvicorn
```

**Erreur** : `Port already in use`

**Solution** :
```bash
# Utiliser un autre port
uvicorn mcp_api_server:app --port 8001
```

### Probl√®me : Mod√®le non trouv√©

**Erreur** : `OSError: Can't load tokenizer`

**Solution** :

1. V√©rifiez le chemin :
   ```python
   import os
   MODEL_PATH = r"C:\Users\HP ZBOOK\Desktop\ETUDES\2024-2025\NLP\models\twitter-xlm-roberta"
   print(os.path.exists(MODEL_PATH))  # Doit afficher True
   ```

2. Re-t√©l√©chargez le mod√®le :
   ```python
   from transformers import AutoTokenizer, AutoModelForSequenceClassification
   
   tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-xlm-roberta-base-sentiment")
   model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-xlm-roberta-base-sentiment")
   
   tokenizer.save_pretrained(MODEL_PATH)
   model.save_pretrained(MODEL_PATH)
   ```

### Probl√®me : Texte trop long

**Erreur** : `Token indices sequence length is longer than the maximum sequence length`

**Ce n'est normalement PAS un probl√®me** car le syst√®me d√©coupe automatiquement. Si cela se produit :

1. V√©rifiez que la fonction `analyze_sentiment` contient bien la logique de chunking
2. V√©rifiez les logs pour voir o√π l'erreur se produit
3. Limitez manuellement la longueur :
   ```python
   text = text[:5000]  # Limite √† ~5000 caract√®res
   ```

### Probl√®me : R√©sultats incoh√©rents

**Causes possibles** :

1. **Texte mal format√©** : V√©rifiez le nettoyage
   ```python
   from mcp_sentiment_server import clean_text
   print(clean_text("Votre texte"))
   ```

2. **Langue incorrecte** : Le mod√®le est optimis√© pour le fran√ßais
   ```python
   # √âvitez les textes en anglais, espagnol, etc.
   ```

3. **Texte trop court** : Minimum 3-5 mots recommand√©s
   ```python
   if len(text.split()) < 3:
       print("Texte trop court pour une analyse fiable")
   ```

---

## üõ†Ô∏è D√©veloppement

### Ajouter une nouvelle fonctionnalit√© MCP

1. Modifiez `handle_mcp_request` dans `mcp_sentiment_server.py`
2. Ajoutez le sch√©ma dans `tools/list`
3. Impl√©mentez la logique dans `tools/call`
4. Red√©marrez Claude Desktop

**Exemple** : Ajouter une fonction de traduction

```python
def handle_mcp_request(request: dict) -> dict:
    method = request.get("method")
    
    if method == "tools/list":
        return {
            "tools": [
                # ... outil existant ...
                {
                    "name": "translate_and_analyze",
                    "description": "Translate text to French and analyze sentiment",
                    "inputSchema": {
                        "type": "object",
                        "properties": {
                            "text": {"type": "string"},
                            "source_lang": {"type": "string"}
                        },
                        "required": ["text"]
                    }
                }
            ]
        }
    
    elif method == "tools/call":
        tool_name = request["params"]["name"]
        
        if tool_name == "translate_and_analyze":
            # Impl√©menter la traduction + analyse
            pass
```

### Tests unitaires

Cr√©ez `test_sentiment.py` :

```python
import pytest
from mcp_sentiment_server import analyze_sentiment, clean_text

def test_clean_text():
    assert clean_text("  HELLO  ") == "hello"
    assert clean_text("<p>Test</p>") == "test"

def test_analyze_positive():
    result = analyze_sentiment({"text": "J'adore ce produit !"})
    assert result["sentiment"] == "positive"
    assert result["confidence"] > 0.5

def test_analyze_negative():
    result = analyze_sentiment({"text": "C'est horrible"})
    assert result["sentiment"] == "negative"

# Ex√©cuter : pytest test_sentiment.py
```

---

## üìä Performance

### Benchmarks

| Taille du texte | Temps d'analyse | M√©moire |
|-----------------|-----------------|---------|
| < 100 mots      | ~0.5s           | ~500 MB |
| 100-500 mots    | ~1.5s           | ~600 MB |
| > 500 mots      | ~3-5s           | ~800 MB |

### Optimisations possibles

1. **Cache des r√©sultats** :
   ```python
   from functools import lru_cache
   
   @lru_cache(maxsize=1000)
   def analyze_sentiment_cached(text):
       return analyze_sentiment({"text": text})
   ```

2. **Batch processing** :
   ```python
   # Analyser plusieurs textes en un seul appel
   def analyze_batch(texts):
       inputs = tokenizer(texts, return_tensors="pt", padding=True)
       with torch.no_grad():
           outputs = model(**inputs)
       # ...
   ```

3. **GPU** (si disponible) :
   ```python
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   model.to(device)
   ```

---

## ü§ù Contribution

### Comment contribuer

1. Forkez le projet
2. Cr√©ez une branche (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add AmazingFeature'`)
4. Pushez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

### Standards de code

- **PEP 8** pour Python
- Docstrings pour toutes les fonctions
- Tests unitaires pour les nouvelles features
- Logging pour debugging

---

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

---

## üë• Auteurs

- **Votre Nom** - D√©veloppement initial

---

## üôè Remerciements

- [Hugging Face](https://huggingface.co/) pour les mod√®les transformers
- [Cardiff NLP](https://cardiffnlp.github.io/) pour le mod√®le XLM-RoBERTa
- [Anthropic](https://www.anthropic.com/) pour Claude et le MCP

---

## üìû Support

Pour toute question ou probl√®me :

- üìß Email : votre.email@example.com
- üêõ Issues : [GitHub Issues](https://github.com/votre-repo/issues)
- üìñ Documentation : [Wiki](https://github.com/votre-repo/wiki)

---

**Version** : 1.0.0  
**Derni√®re mise √† jour** : Janvier 2026